# Spreadsheet Deduplication Pipeline

A **minimal record deduplication workflow** in Python that identifies and merges duplicate entries in spreadsheet data. The pipeline cleans data, generates candidate pairs, computes similarity features, trains a model to score duplicates, and produces an easy-to-review report.

## ğŸš€ Quick Start

1. **Install:** `python -m pip install -r requirements.txt`
2. **Run:** `python -m src.cli.preprocess data/your_file.csv`
3. **Continue:** See [Usage Guide](docs/USAGE.md) for the complete 6-step workflow

## ğŸ“‹ Minimal Data Requirements

Works with just **two columns**:
- **Record ID** (`record_id` or `sys_id`)
- **Company name** (any name/company column)

```csv
record_id,company
1,Acme Corp
2,ACME Corporation  
3,Beta LLC
```

**Optional fields for better accuracy:**
- `domain`/`website` - Domain-based matching
- `phone` - Phone number matching
- `address` - Address similarity
- `state`, `country_code` - Geographic matching

## ğŸ“š Documentation

| Guide | Description |
|-------|-------------|
| **[Installation](docs/INSTALLATION.md)** | Setup, dependencies, virtual environments |
| **[Usage](docs/USAGE.md)** | Step-by-step commands and examples |
| **[Pipeline Steps](docs/PIPELINE_STEPS.md)** | Detailed technical documentation |
| **[GPT Integration](docs/GPT_INTEGRATION.md)** | Optional AI features and setup |
| **[AI Deduplication](docs/AI_DEDUPLICATION.md)** | ğŸ†• AI-powered record merging |

## ğŸ”§ Key Features

âœ… **Minimal data support** - Works with just company names and IDs  
âœ… **Flexible input** - CSV, Excel files (.xlsx, .xls)  
âœ… **Robust error handling** - Clear messages for missing columns  
âœ… **Modular design** - Run individual steps or full pipeline  
âœ… **Experiment tracking** - Comprehensive logging and statistics  
âœ… **Multiple approaches** - Supervised ML + unsupervised clustering  
âœ… **AI-powered deduplication** - ğŸ†• Intelligent record merging with OpenAI  

## âš¡ Pipeline Overview

```mermaid
graph LR
    A[Raw Data] --> B[1. Preprocess]
    B --> C[2. Blocking]
    C --> D[3. Similarity]
    D --> E[4. Model]
    E --> F[5. Reporting]
    D --> G[6. Clustering]
    F --> H[Excel Review]
    G --> I[GPT Analysis]
```

1. **Preprocess** - Clean & normalize data
2. **Blocking** - Generate candidate pairs
3. **Similarity** - Compute feature vectors
4. **Model** - Train ML classifier
5. **Reporting** - Excel output for review
6. **Clustering** - Alternative DBSCAN approach

## ğŸ—‚ï¸ Project Structure

```
record-deduplication/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ your_spreadsheet.csv      # Input data
â”‚   â”œâ”€â”€ sample_input.csv          # Sample for testing
â”‚   â””â”€â”€ ğŸ“ outputs/               # All pipeline outputs
â”‚       â”œâ”€â”€ cleaned.csv           # Preprocessed data
â”‚       â”œâ”€â”€ features.csv          # Similarity features
â”‚       â”œâ”€â”€ high_confidence.csv   # Likely duplicates
â”‚       â””â”€â”€ manual_review.xlsx    # Review spreadsheet
â”œâ”€â”€ ğŸ“ src/                       # Pipeline modules
â”‚   â”œâ”€â”€ preprocess.py            # Data cleaning
â”‚   â”œâ”€â”€ blocking.py              # Candidate generation
â”‚   â”œâ”€â”€ similarity.py            # Feature computation
â”‚   â”œâ”€â”€ model.py                 # ML training/scoring
â”‚   â”œâ”€â”€ reporting.py             # Excel output
â”‚   â””â”€â”€ clustering.py            # DBSCAN clustering
â”œâ”€â”€ ğŸ“ docs/                     # Documentation
â””â”€â”€ ğŸ“ tests/                    # Unit tests
```

## ğŸš¨ Troubleshooting

**Minimal dataset issues:**

| Problem | Solution |
|---------|----------|
| Missing required columns | Ensure `record_id` and `company` columns exist |
| Low accuracy | Add more fields (domain, phone, address) |
| No duplicates found | Lower confidence threshold or review manually |
| API errors (GPT) | Check `OPENAI_KEY` environment variable |

## ğŸ§ª Testing

Run the test suite to verify installation:

```bash
python -m unittest discover
```

## ğŸ“„ License

This project is provided as-is for educational and research purposes.

---

**Need help?** Check the [detailed documentation](docs/) or run with sample data:
```bash
python -m src.cli.preprocess data/sample_input.csv --normalize --deduplicate
```